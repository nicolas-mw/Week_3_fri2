---
title: "Tumor Classifier - K-Nearest Neighbours"
author: "Your name"
date: "September 11, 2024"
output: 
  html_document:
    toc: true
    number_sections: true
editor_options: 
  chunk_output_type: console
---

# Tumour Classifier with K-Nearest Neighbours (KNN)

In this exercise, you'll build a KNN classifier to distinguish between tumor classes based on a subset of features. Complete the steps by filling in the missing code and answering questions.

You will use the `caret` package for this exercise. Use the 'help' in RStudio to discover how they work. The key functions that you will use are:

- createDataPartition
- trainControl
- train
- predict
- confusionMatrix

## Instructions

### Step 1: Install and load necessary libraries

Fill in the missing code to install and load the following libraries: tidyverse, here, and caret.

```{r }
# Install packages if you haven't already
install.packages("caret")

library(caret) 
library(tidyverse)
library(here)

createDataPartition
trainControl
train
predict
confusionMatrix
```

### Step 2: Load the dataset

```{r load-data}
cancer_data <- read_csv(here("data", "synthetic_cancer_data.csv"))
cancer_data
```

### Step 3: Prepare the data

-   Select the three relevant variables; Perimeter, Concavity, Class
-   Convert the target variable to a factor
-   Split the data into 80% training and 20% testing sets

```{r}

# Keep only the three relevant columns
data_subset <- cancer_data %>% 
  select(Perimeter, Concavity, Class)

# Encode the target variable as a factor
data_subset <- data_subset %>% 
  mutate(Class = as.factor(Class))
  
# Set a seed for reproducibility
set.seed(42)

# Split the data into training (80%) and testing (20%) sets
train_index <- createDataPartition(data_subset$Class, p = 0.8, list = FALSE)
## NOTE: caret::createDataPartition() randomly splits the data (which, by setting the seed, is always the same for reproducibility - YOU MUST SET SEED BEFOREHAND) into training and test sets (or other partitions), p = 0.8 means 80% of the data will be saved to the train_index object, list = FALSE means that the output will be a vector of row indices rather than a list

train_data <- data_subset %>% 
  slice(train_index)
## NOTE: dplyr::slice() selects rows from a data frame based on the row indices provided (in this case from the createDataPartition() function), you cannot use the filter() function because this selects rows based on a logical argument - the ouptut must be TRUE or FLASE

test_data <- data_subset %>% 
  slice(-train_index)
## NOTE: the '-' sign before the train_index means that it will select all rows except the ones in the train_index

```

*Question*: Why is it important to set a seed before splitting the data?

*Answer*: For reproduciility - so another person could get the same set of random numbers if they were to run the code

### Step 4: Train the KNN model

-   Define a cross-validation control.
-   Train the KNN model, tuning the number of neighbours 'k' using tuneLength = 10.

```{r}
# Define training control (using 10-fold cross-validation)
trainControl <- trainControl(method = "cv", number = 10 )
# sets the training parameters, folds the the training data into 10 folds and each time trains the data on 9 folds and tests with the remainder fold, repeats this 10 times, this does not train the data just sets the parameter for training

# Train the KNN model 
knn_model <- train(Class ~ ., 
                 data = train_data, 
                 preProcess = c("scale", "centre"),
                 method = "knn",
                 trcontrol = ...,
                 ... = ...,
                 ... = ...)
### (transfer to word doc notes) scaling is required so that no one variable with larger magnitdues can dominate the model, the method is k-nearest neighbours, the number of neighbours is tuned using tuneLength = 10, the metric used to evaluate the model is accuracy
## NOTE: trcontrol = argument is so that you can set the parameters of the train as what you previously defined using the trainControl() function 
### dunno what tunelength does (it defines the amount of k numbers it will test and give in output, it gives the best 10, any more than 10 will take ages to compute and is unlikely to improve the model), and also check the output of the function (the ouput automatically chooses the best value of K for the model)

# View the results
knn_model

```

*Question*: What is the purpose of using cross-validation when training the model?

*Answer*: ...

### Step 5: Make predictions

-   Use the trained model to predict on the test data.
-   Assess the performance using a confusion matrix.

```{r}
# Make predictions on the test set
predictions <- ...(..., ... = ...)
## NOTE: the predict() function is used to make predictions from various model fitting functions on the test data using the trained model, the first argument is the model you want to predict with

# Confusion matrix  
confusionMatrix(predictions, test_data$Class)
## NOTE: a confusion matrix will tell you how many times the model was correct and incorrect (both false positive and false negative)

```

*Question*: What insights can you gather from the confusion matrix? Is the model performing well?

*Answer*: ...

### Step 6: Tune the model (optional)

-   Explore the best value of 'k' from the trained model.
-   Discuss how the number of neighbors (k) impacts model performance.

```{r}
# Check the best value of 'k'
... <- ...$...
print(...)
```

*Question*: What is the optimal value of 'k'? How would you explain its significance?

*Answer*: ...
